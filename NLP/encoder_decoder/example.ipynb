{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data from a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../nlp_data.csv')\n",
    "for line in df.values[:, :-1]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum length of the input and output sequences\n",
    "max_input_len = max(len(line.split(' ')) for line in df.iloc[:, :-1].values.flatten() if isinstance(line, str))\n",
    "max_output_len = max(len(line.split(' ')) for line in df['report'].values if isinstance(line, str))\n",
    "\n",
    "# Print the maximum input and output lengths\n",
    "print('Max input length:', max_input_len)\n",
    "print('Max output length:', max_output_len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "for i in range(len(df)):\n",
    "    input_line = df.iloc[i][:-1].to_string(index=False)\n",
    "    target_line = df.iloc[i]['report']\n",
    "    input_texts.append(input_line)\n",
    "    target_texts.append(target_line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the input and output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokenizer = Tokenizer(filters='', lower=False)\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_input_len, padding='post')\n",
    "num_input_tokens = len(input_tokenizer.word_index) + 1\n",
    "\n",
    "output_tokenizer = Tokenizer(filters='', lower=False)\n",
    "output_tokenizer.fit_on_texts(target_texts)\n",
    "target_sequences = output_tokenizer.texts_to_sequences(target_texts)\n",
    "target_sequences = pad_sequences(target_sequences, maxlen=max_output_len, padding='post')\n",
    "num_output_tokens = len(output_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "latent_dim = 256\n",
    "encoder_inputs = Input(shape=(max_input_len,))\n",
    "encoder_embedding = Embedding(num_input_tokens, latent_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(max_output_len-1,))\n",
    "decoder_embedding = Embedding(num_output_tokens, latent_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_output_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_output_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "model.summary()\n",
    "# Train the model\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "history = model.fit([input_sequences, target_sequences[:, :-1]], target_sequences[:, 1:], batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reports\n",
    "def generate_report(input_sequence):\n",
    "    encoder_input = input_tokenizer.texts_to_sequences([input_sequence])\n",
    "    encoder_input = pad_sequences(encoder_input, maxlen=max_input_len, padding='post')\n",
    "    decoder_input = np.zeros(shape=(1, max_output_len))\n",
    "    decoder_input[0, 0] = output_tokenizer.word_index['<start>']\n",
    "    for i in range(1, max_output_len):\n",
    "        output_tokens = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n",
    "        decoder_input[0, i] = output_tokens[0, i-1]\n",
    "        if output_tokens[0, i] == output_tokenizer.word_index['<end>']:\n",
    "            break\n",
    "    output_sequence = ' '.join(output_tokenizer.index_word[idx] for idx in decoder_input[0] if idx > 0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
